{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vertical Strategy: Use text column to predict tex_paragraph column**\n",
    "\n",
    "**Outline**\n",
    "\n",
    "1. âœ… filter out stopwords in text column\n",
    "    - word tokenize text column\n",
    "2. âœ…  use token match in big string to define candidate paragraphs\n",
    "    - merge overlapping paragraphs\n",
    "3. âœ… Score/rank paragraphs\n",
    "    - some terms will be more relevant/weighted than others\n",
    "    - also score them by requency\n",
    "4. âœ… Check similarity between predicted paragraphs and actual paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertical Pipeline: turn vertical strategy into something easily iterable\n",
    "- like a function callable on an html that returns a nominated paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate parameters from curated training data\n",
    "\n",
    "### read in the vertical training subset data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read in the vertical training subset\n",
    "vert_training_subset_df = pd.read_csv('data/vertical_training_subset.csv', index_col=[0])\n",
    "vert_training_subset_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### these are the only dkfns that I'm currently working with\n",
    "print(set(vert_training_subset_df.data_key_friendly_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate key_tokens from text colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compile a string from the text column\n",
    "\n",
    "text_list = [t.lower() for t in vert_training_subset_df.text]\n",
    "#print('This is what the text_list looks like: ')\n",
    "#print(\"----------------------------\")\n",
    "#print(text_list[0:3])\n",
    "#print(\"----------------------------\")\n",
    "text_string = ''\n",
    "for t in text_list:\n",
    "    text_string = text_string + t + ' '\n",
    "#print(\"\")\n",
    "#print(\"And now this is what the flattened text_string looks like: \")\n",
    "#print(\"----------------------------\")\n",
    "#print(text_string[0:250])\n",
    "#print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize the text string\n",
    "\n",
    "text_tokens = [w for w in word_tokenize(text_string) if w.isalpha()]\n",
    "no_stops = [t for t in text_tokens if t not in stopwords.words('english')]\n",
    "#print(no_stops[0:10])\n",
    "\n",
    "### limit ourselves to the 30 most common tokens\n",
    "    # although this can be modulated if necessary\n",
    "key_tokens = pd.DataFrame(Counter(no_stops).most_common(30))\n",
    "key_tokens.columns = ['key_tok', 'kt_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can probably improve the above list of key_tokens by using less than 30 terms.... maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ¤” I could enhance my key_tokens by doing tf-idf\n",
    "- between the text column and the the html text - the text column\n",
    "    - which terms are prominent in text column that are not prominent in the rest of the html\n",
    "    - which terms are prominent in the noisy signal that are not prominent in the noise?\n",
    "- I could also enhance it by removing noisy words like 'total'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define paragraph margins\n",
    "as the standard deviation of paragraph lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_lengths = [len(par) for par in vert_training_subset_df.paragraph_text]\n",
    "margin = int(np.std(par_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will be the beginning of the loop\n",
    "1. read in a filing\n",
    "2. call a function that\n",
    "    - defines candidate paragraphs\n",
    "    - ranks or scores candidate paragraphs to select top nominee(s)\n",
    "    - validates if predicted nominee is a good match to actual paragraph in the training csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage filings to be read in iterably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a list of paths\n",
    "training_filings_list = []\n",
    "\n",
    "# populate the list\n",
    "for root, dirs, files in os.walk('data/nc_training_filings/'):\n",
    "    training_filings_list += glob.glob(os.path.join(root, '*.html'))\n",
    "#print(training_filings_list[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# need to isolate only those filings which are in my vertical training df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(vert_training_subset_df.accession_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(['data/nc_training_filings/' + an + '.html' for an in vert_training_subset_df.accession_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_training_subset_filings_list = list(set(['data/nc_training_filings/' + an + '.html' for an in vert_training_subset_df.accession_number]))\n",
    "vert_training_subset_filings_list\n",
    "assert len(vert_training_subset_filings_list) == len(set(vert_training_subset_df.accession_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vert_training_subset_filings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for now, lets practice on only three filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulomartinez/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /Users/paulomartinez/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  26\n",
      "that is a  78.79 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  35\n",
      "that is a  53.85 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  76\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  45\n",
      "that is a  53.57 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  18\n",
      "that is a  45.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  57\n",
      "that is a  45.24 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  85\n",
      "that is a  77.27 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  100\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  66\n",
      "that is a  71.74 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  250\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  104\n",
      "that is a  99.05 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  60\n",
      "that is a  57.14 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  111\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  125\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  39\n",
      "that is a  47.56 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  73\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  139\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  46\n",
      "that is a  76.67 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  144\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  38\n",
      "that is a  42.7 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  144\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  52\n",
      "that is a  50.98 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  61\n",
      "that is a  57.55 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  1\n",
      "that is a  2.04 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  87\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  41\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  18\n",
      "that is a  50.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  42\n",
      "that is a  46.67 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  46\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  81\n",
      "that is a  94.19 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  174\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  55\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  67\n",
      "that is a  76.14 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  242\n",
      "that is a  85.82 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  130\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  36\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  58\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  22\n",
      "that is a  20.56 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  46\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  26\n",
      "that is a  41.27 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  23\n",
      "that is a  46.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  37\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  37\n",
      "that is a  59.68 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  52\n",
      "that is a  50.98 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  99\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  205\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  254\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  130\n",
      "that is a  98.48 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  21\n",
      "that is a  67.74 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  54\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  12\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  143\n",
      "that is a  89.94 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  162\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  68\n",
      "that is a  47.55 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  68\n",
      "that is a  67.33 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  66\n",
      "that is a  55.93 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  42\n",
      "that is a  68.85 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  21\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  96\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  105\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  93\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  41\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  46\n",
      "that is a  44.23 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  103\n",
      "that is a  99.04 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  17\n",
      "that is a  94.44 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  39\n",
      "that is a  56.52 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  53\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  103\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  91\n",
      "that is a  62.76 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  219\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  56\n",
      "that is a  93.33 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  53\n",
      "that is a  73.61 % inclusion\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  35\n",
      "that is a  64.81 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  175\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  59\n",
      "that is a  67.82 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  63\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  124\n",
      "that is a  99.2 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  77\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  136\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  86\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  106\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  57\n",
      "that is a  65.52 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  66\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  36\n",
      "that is a  57.14 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  62\n",
      "that is a  53.45 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  45\n",
      "that is a  45.45 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  53\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  60\n",
      "that is a  81.08 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  46\n",
      "that is a  51.69 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  49\n",
      "that is a  76.56 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  73\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  84\n",
      "that is a  94.38 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  53\n",
      "that is a  59.55 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  50\n",
      "that is a  58.82 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  37\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  108\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  104\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  163\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- False\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  52\n",
      "that is a  91.23 % inclusion\n",
      "-------------------------------------------------\n",
      "\n",
      "Actual_paragraph in nom_validatable: \n",
      "---------- True\n",
      "word tokens actual_paragraph also in word_tokenise(nom_validatable)\n",
      "---------- counter =  125\n",
      "that is a  100.0 % inclusion\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize some score keeping lists\n",
    "total_inclusion = []\n",
    "inclusion_rates = []\n",
    "\n",
    "#for filing in training_filings_list:\n",
    "for filing in vert_training_subset_filings_list[0:100]:\n",
    "    \n",
    "    \n",
    "### this could be all be packed into a function if nec.\n",
    "###\n",
    "    # read in the filing's html\n",
    "    with open(filing) as file:\n",
    "        file_html = file.read()\n",
    "    \n",
    "    # parse the html\n",
    "    parsed_html = BeautifulSoup(file_html)\n",
    "    # filter out the html head, css, and cast to lower case\n",
    "    string_filing = parsed_html.body.text.lower()\n",
    "    \n",
    "    ################################################################\n",
    "    #define candidate paragraphs by clustering of overlapping token-hit-margins\n",
    "    #\n",
    "    #------ generate a list of token mathces / \"hits\"------------------------------\n",
    "    #\n",
    "    # collect the start and end indices of each token \"hit.\" into an ugly list of lists of tuples\n",
    "    lst_of_token_hits_lst = []\n",
    "    for i in range(len(key_tokens.key_tok[0:10])):\n",
    "        kt = key_tokens.key_tok[i]\n",
    "        lst_of_token_hits_lst.append([(str(kt), m.start(), m.end()) for m in re.finditer(kt, string_filing)])\n",
    "    #\n",
    "    # flatten the ugly list of lists of tuples into a list of tuples\n",
    "    flater_list_of_tokhits = [tpl for sublist in lst_of_token_hits_lst for tpl in sublist]\n",
    "    #\n",
    "    # order the list of token_hit tuples by starting index\n",
    "    flater_list_of_tokhits.sort(key = lambda x: x[1])\n",
    "    #\n",
    "    #-------- use the token hits to define the bounds of the candidate paragraphs -------\n",
    "    #\n",
    "    # initialize a list of candidate paragraphs and a hit tracker\n",
    "    candidate_paragraphs = []\n",
    "    hit = 0\n",
    "    #\n",
    "    # work through the hits until I exhaust the list\n",
    "    while hit < len(flater_list_of_tokhits) - 1:\n",
    "    #    \n",
    "        # initialize the left and right bound of a paragraph\n",
    "        leftbound = flater_list_of_tokhits[hit][1] - margin\n",
    "        rightbound = flater_list_of_tokhits[hit][2] + margin\n",
    "    #     \n",
    "        # loop from the hit + 1 (cause we used the former hit to initialize the bounds)\n",
    "        for i in range(hit + 1, len(flater_list_of_tokhits)):\n",
    "            # if the next token's start is within the current paragraph's bounds and it isnt the last hit\n",
    "            if (leftbound < flater_list_of_tokhits[i][1] < rightbound) and (i != len(flater_list_of_tokhits) - 1):\n",
    "                # expand the right bound with respect to next token's end\n",
    "                rightbound = flater_list_of_tokhits[i][2] + margin\n",
    "    #             \n",
    "            # if next token is outside the bounds of current paragraph or we are at the penultimate hit\n",
    "            else:\n",
    "                # if right bound is out of htmls range, then adjust it to the end of the html\n",
    "                if rightbound > len(string_filing):\n",
    "                    rightbound = len(string_filing)\n",
    "                # define current paragraph\n",
    "                par = string_filing[leftbound:rightbound] ########### (if the code needs to be made more efficient I could store the paragraph start and end indices instead of storing the strings)\n",
    "                # store the paragraph in a list of candidates\n",
    "                candidate_paragraphs.append(par)\n",
    "                # note which hit we need to start on for the next iteration of the while loop\n",
    "                hit = i\n",
    "                # now that we've found the outer bounds of the current paragraph we can interrupt the for loop\n",
    "                break #this won't interrupt the while loop ðŸ˜Š \n",
    "    #\n",
    "    # this is just a repetition of the above routine to handle the last hit. There is probably a better way to handle this but we can enhance that later\n",
    "    if hit == len(flater_list_of_tokhits)-1:\n",
    "        leftbound = flater_list_of_tokhits[hit][1] - margin\n",
    "        rightbound = flater_list_of_tokhits[hit][2] + margin\n",
    "        if rightbound > len(string_filing):\n",
    "            rightbound = len(string_filing)\n",
    "        par = string_filing[leftbound:rightbound]\n",
    "        candidate_paragraphs.append(par)\n",
    "    #\n",
    "    # --------------- store the candidate paragraphs in a df ------------------\n",
    "    candidate_df = pd.DataFrame(candidate_paragraphs)\n",
    "    candidate_df.columns = ['cand_par']\n",
    "    candidate_df['len'] = [len(par) for par in candidate_df.cand_par]\n",
    "    # --------------- rank the candidate paragraphs by basic length (longer paragraphs correspond to denser clusters)\n",
    "    candidate_df.sort_values(by = 'len', ascending = False, inplace = True)\n",
    "    candidate_df.reset_index(inplace = True, drop = True)\n",
    "    # --------------- filter candidates downto those with 'repurchase' in them\n",
    "    candidate_df = candidate_df[['repurchase' in par for par in candidate_df.cand_par]]\n",
    "    candidate_df.reset_index(inplace = True, drop = True)\n",
    "    # --------------- nominee(s) will be the top 1 - 3 in the filterd df\n",
    "    nominated = candidate_df.cand_par[0]\n",
    "    # clean up the nominee for validation\n",
    "    nom_validatable = re.sub(r'\\n+', ' ', nominated) # remove new lines\n",
    "    nom_validatable = re.sub(r'\\s+', ' ', nom_validatable) # trim all repeated whitespace down to one\n",
    "    # --------------- isolate csv's actual paragraph for valiation\n",
    "    # extract the accesion number back out of the filing path\n",
    "    an = re.sub('data/nc_training_filings/', '', filing)\n",
    "    an = re.sub('.html', '', an)\n",
    "    # use it to filter down to the csv rows with that an\n",
    "    actual_paragraph = vert_training_subset_df[vert_training_subset_df.accession_number == an].head(1).paragraph_text\n",
    "    # extract the string from the resulting pandas series\n",
    "    actual_paragraph = actual_paragraph.values[0]\n",
    "    # cast the string to lower case\n",
    "    actual_paragraph = actual_paragraph.lower()\n",
    "    # eliminate excess spaces\n",
    "    actual_paragraph = re.sub(r'\\s+', ' ', actual_paragraph)\n",
    "    \n",
    "    # moment of truth\n",
    "    print('Actual_paragraph in nom_validatable: ')\n",
    "    print('----------', actual_paragraph in nom_validatable)\n",
    "    \n",
    "    counter = 0\n",
    "    for wt in word_tokenize(actual_paragraph):\n",
    "        if wt in word_tokenize(nom_validatable):\n",
    "            counter += 1\n",
    "    print('word tokens actual_paragraph also in word_tokenise(nom_validatable)')\n",
    "    print('---------- counter = ', counter)\n",
    "    x = round(100*counter/len(word_tokenize(actual_paragraph)),2)\n",
    "    print('that is a ', x, '% inclusion')\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # keeping score\n",
    "    total_inclusion.append(actual_paragraph in nom_validatable)\n",
    "    inclusion_rates.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, True, False, False, False, False, False, False, True, False, False, True, True, False, True, True, False, True, False, True, False, False, False, True, True, False, False, False, False, True, True, False, False, True, True, False, False, True, False, False, True, False, False, True, True, True, False, False, True, True, False, True, False, False, False, False, True, True, True, False, True, False, True, False, False, True, False, False, True, False, False, False, True, False, True, False, True, True, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False, True, True, True, True, False, True]\n"
     ]
    }
   ],
   "source": [
    "print(total_inclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(total_inclusion).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.0 % total inclusion\n"
     ]
    }
   ],
   "source": [
    "print(100*np.array(total_inclusion).sum()/len(total_inclusion), '% total inclusion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78.79, 53.85, 100.0, 53.57, 45.0, 45.24, 77.27, 100.0, 71.74, 100.0, 99.05, 57.14, 100.0, 100.0, 47.56, 100.0, 100.0, 76.67, 100.0, 42.7, 100.0, 50.98, 57.55, 2.04, 100.0, 100.0, 50.0, 46.67, 100.0, 94.19, 100.0, 100.0, 76.14, 85.82, 100.0, 100.0, 100.0, 20.56, 100.0, 41.27, 46.0, 100.0, 59.68, 50.98, 100.0, 100.0, 100.0, 98.48, 67.74, 100.0, 100.0, 89.94, 100.0, 47.55, 67.33, 55.93, 68.85, 100.0, 100.0, 100.0, 100.0, 100.0, 44.23, 99.04, 94.44, 56.52, 100.0, 100.0, 62.76, 100.0, 93.33, 73.61, 64.81, 100.0, 67.82, 100.0, 99.2, 100.0, 100.0, 100.0, 100.0, 65.52, 100.0, 57.14, 53.45, 45.45, 100.0, 81.08, 51.69, 76.56, 100.0, 94.38, 59.55, 58.82, 100.0, 100.0, 100.0, 100.0, 91.23, 100.0]\n"
     ]
    }
   ],
   "source": [
    "print(inclusion_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  0.,  0., 13., 12.,  7.,  7.,  3., 56.]),\n",
       " array([  2.04 ,  11.836,  21.632,  31.428,  41.224,  51.02 ,  60.816,\n",
       "         70.612,  80.408,  90.204, 100.   ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(inclusion_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For now, ranking paragraphs by something as unrefined as brute length is working well enough\n",
    "### filtering the candidate paragraphs to those containing 'repurchase' worked well too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” ðŸ’¡ \n",
    "#### can enhance by counting how many key_token hits each paragraph has\n",
    "#### can enhance by assigning weighted values to certain key_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
